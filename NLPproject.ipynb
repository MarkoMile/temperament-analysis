{"cells":[{"cell_type":"code","execution_count":27,"metadata":{"executionInfo":{"elapsed":14170,"status":"ok","timestamp":1660489158953,"user":{"displayName":"Marko Milenković","userId":"00943616786887211077"},"user_tz":-120},"id":"vw4zrN3dhVxh"},"outputs":[],"source":["import pandas as pd\n","import spacy\n","import re\n","import tldextract\n","from sklearn.preprocessing import OneHotEncoder\n","import numpy as np\n","\n","\n","\n","nRowsRead = 5  # specify 'None' if want to read whole file\n","postStrings = []\n","typeStrings = []"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":409,"status":"ok","timestamp":1660489731537,"user":{"displayName":"Marko Milenković","userId":"00943616786887211077"},"user_tz":-120},"id":"YmgbW9Ax3QmE"},"outputs":[],"source":["df1 = pd.read_csv('./input/mbti_1.csv', delimiter=',', nrows=nRowsRead)\n","df1.dataframeName = 'mbti_1.csv'"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1660489735283,"user":{"displayName":"Marko Milenković","userId":"00943616786887211077"},"user_tz":-120},"id":"me0evtXa3ScU"},"outputs":[],"source":["# making array of array of posts\n","for person,type in zip(df1['posts'],df1['type']):\n","    postStrings.append([type,person.split('|||')])\n","\n","# removing excess single parentheses\n","\n","for i in range(0,len(postStrings)):\n","    postStrings[i][1][0] = postStrings[i][1][0][1:]\n","    postStrings[i][1][-1] = postStrings[i][1][-1][:-1]"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["URL_REGEX = r\"\"\"(?i)\\b((?:[a-z][\\w-]+:(?:/{1,3}|[a-z0-9%])|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))\"\"\"\n","DOMAIN_REGEX = \"^(?:https?:\\/\\/)?(?:[^@\\/\\n]+@)?(?:www\\.)?([^:\\/?\\n]+)\"\n","COMMENT_REGEX = r\"\\[.*?\\]\"\n","ELLIPSIS_REGEX = r\"^(\\ *\\.{3}\\ *)|(\\ *\\.{3}\\ *)$\"\n","\n","def urlReplace(stringToReplace):\n","\tdomain = tldextract.extract(stringToReplace.group())\n","\treturn domain.domain + '.' + domain.suffix\n"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["def containsOnlyUrlsOrNumbers(post):\n","\tsplits = post.split()\n","\tfor str in splits:\n","\t\tif(not re.search(URL_REGEX,str) and not re.search('^\\d+$',str) and not re.search('^\\.\\.\\.+$',str)):\n","\t\t\treturn 1\n","\treturn 0\n","\n","def containsComments(post):\n","\tsplits = post.split('] ')\n","\tfor str in splits:\n","\t\tif(not re.search(COMMENT_REGEX,str)):\n","\t\t\treturn 1\n","\t\treturn 0\n","\n","def removeEllipsis(post):\n","\treturn re.sub(ELLIPSIS_REGEX,'',post)\n","\n","def removeWhitespace(post):\n","\tpost = \" \".join(post.split())\n","\treturn post\n","\n","def formatPerson(person):\n","\tperson[1] = list(map(lambda x : removeWhitespace(x),person[1]))\n","\tperson[1] = [post for post in person[1] if (\n","\t\tcontainsOnlyUrlsOrNumbers(post) and containsComments(post))]\n","\tperson[1] = [removeEllipsis(post) for post in person[1]]\n","\tperson[1] = list(map(lambda x : re.sub(URL_REGEX, urlReplace, x),person[1]))\n","\treturn person\n","\n","# for person in postStrings:\n","# \tfor i in range(0, len(person)):\n","# \t\tperson[i]=\" \".join(person[i].split())\n","# \tperson=[post for post in person if (containsOnlyUrlsOrNumbers(post) and containsComments(post))]\n","# \tperson=[removeEllipsis(post) for post in person]\n","# \tfor i in range(0, len(person)):\n","# \t\tperson[i]=re.sub(URL_REGEX, urlReplace, person[i])\n","\n","postStrings = list(map(formatPerson,postStrings))\n","# print(postStrings)\n","# for person in postStrings:\n","# \tprint(person)\n","# \tprint(\"================\")\n","# \tfor post in person:\n","# \t\tprint(post)\n","# \t\tprint(\"================================\")\n"]},{"cell_type":"code","execution_count":33,"metadata":{"executionInfo":{"elapsed":1934,"status":"ok","timestamp":1660490778787,"user":{"displayName":"Marko Milenković","userId":"00943616786887211077"},"user_tz":-120},"id":"4lDomPao3UXN"},"outputs":[{"ename":"ValueError","evalue":"Found array with 0 sample(s) (shape=(0, 1)) while a minimum of 1 is required.","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[1;32m/workspaces/temperament-analysis/NLPproject.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bmarkomile-temperament-analysis-7qxgp9j4hprv4/workspaces/temperament-analysis/NLPproject.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m tokenizedPosts \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(tokenizedPosts)\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bmarkomile-temperament-analysis-7qxgp9j4hprv4/workspaces/temperament-analysis/NLPproject.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m personalityTypes \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(personalityTypes)\n\u001b[0;32m---> <a href='vscode-notebook-cell://codespaces%2Bmarkomile-temperament-analysis-7qxgp9j4hprv4/workspaces/temperament-analysis/NLPproject.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m ohe \u001b[39m=\u001b[39m OneHotEncoder(drop\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mif_binary\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39;49mfit(tokenizedPosts\u001b[39m.\u001b[39;49mreshape(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39m1\u001b[39;49m),personalityTypes)\n","File \u001b[0;32m/opt/python/3.10.4/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:818\u001b[0m, in \u001b[0;36mOneHotEncoder.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    801\u001b[0m \u001b[39mFit OneHotEncoder to X.\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    815\u001b[0m \u001b[39m    Fitted encoder.\u001b[39;00m\n\u001b[1;32m    816\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_keywords()\n\u001b[0;32m--> 818\u001b[0m fit_results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(\n\u001b[1;32m    819\u001b[0m     X,\n\u001b[1;32m    820\u001b[0m     handle_unknown\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle_unknown,\n\u001b[1;32m    821\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    822\u001b[0m     return_counts\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_infrequent_enabled,\n\u001b[1;32m    823\u001b[0m )\n\u001b[1;32m    824\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_infrequent_enabled:\n\u001b[1;32m    825\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_infrequent_category_mapping(\n\u001b[1;32m    826\u001b[0m         fit_results[\u001b[39m\"\u001b[39m\u001b[39mn_samples\u001b[39m\u001b[39m\"\u001b[39m], fit_results[\u001b[39m\"\u001b[39m\u001b[39mcategory_counts\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    827\u001b[0m     )\n","File \u001b[0;32m/opt/python/3.10.4/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:80\u001b[0m, in \u001b[0;36m_BaseEncoder._fit\u001b[0;34m(self, X, handle_unknown, force_all_finite, return_counts)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_n_features(X, reset\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     79\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_feature_names(X, reset\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 80\u001b[0m X_list, n_samples, n_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_X(\n\u001b[1;32m     81\u001b[0m     X, force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite\n\u001b[1;32m     82\u001b[0m )\n\u001b[1;32m     83\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_ \u001b[39m=\u001b[39m n_features\n\u001b[1;32m     85\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcategories \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m\"\u001b[39m:\n","File \u001b[0;32m/opt/python/3.10.4/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:45\u001b[0m, in \u001b[0;36m_BaseEncoder._check_X\u001b[0;34m(self, X, force_all_finite)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[39mPerform custom check_array:\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[39m- convert list of strings to object dtype\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     41\u001b[0m \n\u001b[1;32m     42\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mhasattr\u001b[39m(X, \u001b[39m\"\u001b[39m\u001b[39miloc\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mgetattr\u001b[39m(X, \u001b[39m\"\u001b[39m\u001b[39mndim\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m0\u001b[39m) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m):\n\u001b[1;32m     44\u001b[0m     \u001b[39m# if not a dataframe, do normal check_array validation\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m     X_temp \u001b[39m=\u001b[39m check_array(X, dtype\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite)\n\u001b[1;32m     46\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(X, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m np\u001b[39m.\u001b[39missubdtype(X_temp\u001b[39m.\u001b[39mdtype, np\u001b[39m.\u001b[39mstr_):\n\u001b[1;32m     47\u001b[0m         X \u001b[39m=\u001b[39m check_array(X, dtype\u001b[39m=\u001b[39m\u001b[39mobject\u001b[39m, force_all_finite\u001b[39m=\u001b[39mforce_all_finite)\n","File \u001b[0;32m/opt/python/3.10.4/lib/python3.10/site-packages/sklearn/utils/validation.py:909\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    907\u001b[0m     n_samples \u001b[39m=\u001b[39m _num_samples(array)\n\u001b[1;32m    908\u001b[0m     \u001b[39mif\u001b[39;00m n_samples \u001b[39m<\u001b[39m ensure_min_samples:\n\u001b[0;32m--> 909\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    910\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m sample(s) (shape=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m) while a\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    911\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m minimum of \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m is required\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    912\u001b[0m             \u001b[39m%\u001b[39m (n_samples, array\u001b[39m.\u001b[39mshape, ensure_min_samples, context)\n\u001b[1;32m    913\u001b[0m         )\n\u001b[1;32m    915\u001b[0m \u001b[39mif\u001b[39;00m ensure_min_features \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m array\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m    916\u001b[0m     n_features \u001b[39m=\u001b[39m array\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n","\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 1)) while a minimum of 1 is required."]}],"source":["nlp = spacy.load('en_core_web_sm')\n","\n","\n","# docs = list(nlp.pipe(postStrings[0], n_process=-1))\n","\n","docs = [[person[0],list(nlp.pipe(person[1], n_process=-1))] for person in postStrings]\n","\n","# print([nlp.pipe(person,n_process=-1) for person in postStrings][0])\n","\n","# print([token.text for token in docs[0][2]])\n","\n","tokenizedPosts = []\n","personalityTypes = []\n","\n","for person in docs:\n","  for post in person[1]:\n","    personalityTypes.append(person[0])\n","    tokenizedPosts.append([token.text for token in post])\n","    # tokenizedPosts.append((person[0],[token.text for token in post]))\n","\n","tokenizedPosts = np.asarray(tokenizedPosts)\n","personalityTypes = np.asarray(personalityTypes)\n","\n","ohe = OneHotEncoder(drop='if_binary').fit(personalityTypes.reshape(-1, 1),tokenizedPosts)\n","# transformed = ohe.fit(tokenizedPosts)\n","\n","# print(transformed.toarray())\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPipjDsvaeWPmm+I2AKTUv7","collapsed_sections":[],"mount_file_id":"1_TEjG8vJVKTW0yXQp99YYob3Mif_LgsQ","name":"NLPproject.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3.10.4 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"},"vscode":{"interpreter":{"hash":"4f946df053fbf2b937619d3c5458e7af74262f9a954d8797ba0b27400bcafe06"}}},"nbformat":4,"nbformat_minor":0}
