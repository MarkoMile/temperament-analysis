{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 14170,
          "status": "ok",
          "timestamp": 1660489158953,
          "user": {
            "displayName": "Marko Milenković",
            "userId": "00943616786887211077"
          },
          "user_tz": -120
        },
        "id": "vw4zrN3dhVxh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "import re\n",
        "import tldextract\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "STOP_WORDS = nlp.Defaults.stop_words\n",
        "\n",
        "nRowsRead = 500  # specify 'None' if want to read whole file\n",
        "postStrings = []\n",
        "typeStrings = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 409,
          "status": "ok",
          "timestamp": 1660489731537,
          "user": {
            "displayName": "Marko Milenković",
            "userId": "00943616786887211077"
          },
          "user_tz": -120
        },
        "id": "YmgbW9Ax3QmE"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('./input/mbti_1.csv', delimiter=',', nrows=nRowsRead)\n",
        "df.dataframeName = 'mbti_1.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 4,
          "status": "ok",
          "timestamp": 1660489735283,
          "user": {
            "displayName": "Marko Milenković",
            "userId": "00943616786887211077"
          },
          "user_tz": -120
        },
        "id": "me0evtXa3ScU"
      },
      "outputs": [],
      "source": [
        "# making array of array of posts\n",
        "for person,type in zip(df['posts'],df['type']):\n",
        "    postStrings.append([type,person.split('|||')])\n",
        "\n",
        "# removing excess single parentheses\n",
        "\n",
        "for i in range(0,len(postStrings)):\n",
        "    postStrings[i][1][0] = postStrings[i][1][0][1:]\n",
        "    postStrings[i][1][-1] = postStrings[i][1][-1][:-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#splitting posts into list of posts\n",
        "df['posts'] = df['posts'].apply(lambda x: x.split('|||'))\n",
        "\n",
        "# removing excess single parentheses\n",
        "def removeParentheses(posts):\n",
        "    posts[0] = posts[0][1:]\n",
        "    posts[-1] = posts[-1][:-1]\n",
        "    return posts\n",
        "df['posts'] = df['posts'].apply(lambda x: removeParentheses(x))\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#label encoding type\n",
        "\n",
        "def labelEncodeType(type):\n",
        "  if type == 'ISTJ' or type == 'ISFJ' or type == 'ESTJ' or type == 'ESFJ':\n",
        "    type = 0 #GUARDIAN\n",
        "  elif type == 'ISTP' or type == 'ISFP' or type == 'ESTP' or type == 'ESFP':\n",
        "    type = 1 #ARTISAN\n",
        "  elif type == 'INFJ' or type == 'INFP' or type == 'ENFP' or type == 'ENFJ':\n",
        "    type = 2  # IDEALIST\n",
        "  elif type == 'INTJ' or type == 'INTP' or type == 'ENTP' or type == 'ENTJ':\n",
        "    type = 3  # RATIONALIST\n",
        "  return type\n",
        "\n",
        "df['type'] = df['type'].apply(lambda x: labelEncodeType(x))\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['type'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "URL_REGEX = r\"\"\"(?i)\\b((?:[a-z][\\w-]+:(?:/{1,3}|[a-z0-9%])|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))\"\"\"\n",
        "DOMAIN_REGEX = \"^(?:https?:\\/\\/)?(?:[^@\\/\\n]+@)?(?:www\\.)?([^:\\/?\\n]+)\"\n",
        "COMMENT_REGEX = r\"\\[.*?\\]\"\n",
        "ELLIPSIS_REGEX = r\"^(\\ *\\.{3}\\ *)|(\\ *\\.{3}\\ *)$\"\n",
        "\n",
        "def urlReplace(stringToReplace):\n",
        "\tdomain = tldextract.extract(stringToReplace.group())\n",
        "\treturn domain.domain + '.' + domain.suffix\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**CONSIDER TRYING WITHOUT STOPWORD REMOVAL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def containsOnlyUrlsOrNumbers(post):\n",
        "\tsplits = post.split()\n",
        "\tfor str in splits:\n",
        "\t\tif(not re.search(URL_REGEX,str) and not re.search('^\\d+$',str) and not re.search('^\\.\\.\\.+$',str)):\n",
        "\t\t\treturn 1\n",
        "\treturn 0\n",
        "\n",
        "def containsComments(post):\n",
        "\tsplits = post.split('] ')\n",
        "\tfor str in splits:\n",
        "\t\tif(not re.search(COMMENT_REGEX,str)):\n",
        "\t\t\treturn 1\n",
        "\t\treturn 0\n",
        "\n",
        "def removeEllipsis(post):\n",
        "\treturn re.sub(ELLIPSIS_REGEX,'',post)\n",
        "\n",
        "def removeWhitespace(post):\n",
        "\tpost = \" \".join(post.split())\n",
        "\treturn post\n",
        "\n",
        "def removeStopWords(post):\n",
        "\tpost = \" \".join([t for t in post.split() if t not in STOP_WORDS])\n",
        "\treturn post\n",
        "\n",
        "def formatPerson(person):\n",
        "\tperson = list(map(lambda x: removeWhitespace(x), person))\n",
        "\tperson = [post for post in person if (\n",
        "\t\tcontainsOnlyUrlsOrNumbers(post) and containsComments(post))]\n",
        "\tperson = [removeEllipsis(post) for post in person]\n",
        "\tperson = list(map(lambda x: re.sub(URL_REGEX, urlReplace, x), person))\n",
        "\tperson = list(map(lambda x: removeStopWords(x),person))\n",
        "\treturn person\n",
        "\n",
        "df['posts']= df['posts'].apply(lambda x:formatPerson(x))\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 1934,
          "status": "ok",
          "timestamp": 1660490778787,
          "user": {
            "displayName": "Marko Milenković",
            "userId": "00943616786887211077"
          },
          "user_tz": -120
        },
        "id": "4lDomPao3UXN"
      },
      "outputs": [],
      "source": [
        "#do the nlp tokenization with spaCy\n",
        "\n",
        "def convertToLemmas(posts):\n",
        "  docs=nlp.pipe(posts,n_process=-1)\n",
        "  lemmaList = []\n",
        "  postList = []\n",
        "  for doc in docs:\n",
        "    for token in doc:\n",
        "      lemma = str(token.lemma_)\n",
        "      if lemma == '-PRON-' or lemma == 'be':\n",
        "        lemma = token.text\n",
        "      lemmaList.append(lemma)\n",
        "    postList.append(\" \".join(lemmaList))\n",
        "    lemmaList= []\n",
        "  return postList\n",
        "\n",
        "#for the dataframe:\n",
        "df['posts']= df['posts'].apply(lambda x:convertToLemmas(x))\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#SPLITTING POSTS INTO SEPERATE ROWS\n",
        "df = df.explode('posts')\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['type'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#common words removal\n",
        "freq_comm = pd.Series(' '.join(df['posts'].tolist()).split()).value_counts()\n",
        "f20= freq_comm[:20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['posts'] = df['posts'].apply(lambda x: ' '.join([t for t in x.split() if t not in f20]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#rare words removal\n",
        "rare= freq_comm[freq_comm.values==1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['posts'] = df['posts'].apply(lambda x: ' '.join([t for t in x.split() if t not in rare]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df0 = df[df['type'] == 0].sample(10000)\n",
        "df1 = df[df['type'] == 1].sample(10000)\n",
        "df2 = df[df['type'] == 2].sample(10000)\n",
        "df3 = df[df['type'] == 3].sample(10000)\n",
        "\n",
        "dfr = pd.concat([df0,df1,df2,df3])\n",
        "dfr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.to_csv(path=\"./picke_files/df.pkl\")\n",
        "dfr.to_csv(path=\"./picke_files/dfr.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y = dfr['type']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cv = CountVectorizer()\n",
        "text_counts = cv.fit_transform(dfr['posts'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text_counts.toarray().shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dfr_bow = pd.DataFrame(text_counts.toarray(),columns=cv.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dfr_bow.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dfr_bow.to_csv(path=\"./picke_files/dfr_bow.pkl\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ML ALGORITHMS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rfc = RandomForestClassifier(random_state=42,n_jobs=-1,n_estimators=200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "def classify(X,y):\n",
        "  scaler = MinMaxScaler(feature_range=(0,1))\n",
        "  X=scaler.fit_transform(X)\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X,y,test_size= 0.2,random_state=42,stratify=y)\n",
        "\n",
        "  rfc.fit(X_train,y_train)\n",
        "  y_pred = rfc.predict(X_test)\n",
        "  ac = accuracy_score(y_test,y_pred)\n",
        "  print(\"accuracy: \",ac)\n",
        "\n",
        "\n",
        "classify(dfr_bow,y)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPipjDsvaeWPmm+I2AKTUv7",
      "collapsed_sections": [],
      "mount_file_id": "1_TEjG8vJVKTW0yXQp99YYob3Mif_LgsQ",
      "name": "NLPproject.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
