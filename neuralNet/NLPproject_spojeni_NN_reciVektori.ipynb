{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 14170,
          "status": "ok",
          "timestamp": 1660489158953,
          "user": {
            "displayName": "Marko MilenkoviÄ‡",
            "userId": "00943616786887211077"
          },
          "user_tz": -120
        },
        "id": "vw4zrN3dhVxh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import numpy as np\n",
        "import spacy\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import os\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
        "\n",
        "nlp = spacy.load('en_core_web_lg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# df.to_pickle(\"./pickle_files/df_spojeni_vectors.pkl\")\n",
        "# df = pd.read_pickle(\"./pickle_files/df_spojeni_vectors.pkl\")\n",
        "# df.to_pickle(\"./pickle_files/df_odvojeni_vectors.pkl\")\n",
        "# df = pd.read_pickle(\"./pickle_files/df_odvojeni_vectors.pkl\")\n",
        "# df.to_pickle(\"./pickle_files/df_odvojeni_word_vectors.pkl\")\n",
        "df = pd.read_pickle(\"./pickle_files/df_odvojeni_word_vectors.pkl\")\n",
        "# df = df.head(300000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#spajanje odvojenih postova\n",
        "\n",
        "df = df.groupby(['type', 'avgWordLen'], sort=False,as_index=False).agg({'posts':' '.join,'upperCount':'first','stopWordCount':'first','urlCount':'first','wordCount':'first','avgWordLen':'first','vectors':lambda x:np.array(x)})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# df0 = df[df['type'] == 0].sample(450)\n",
        "# df1 = df[df['type'] == 1].sample(450)\n",
        "# df2 = df[df['type'] == 2].sample(450)\n",
        "# df3 = df[df['type'] == 3].sample(450)\n",
        "\n",
        "# df = pd.concat([df0, df1, df2, df3],)\n",
        "# df = df.reset_index().drop('index', axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#padding vektora\n",
        "def paddingEmbedded(a):\n",
        "  b = np.zeros([len(a), len(max(a, key=lambda x: len(x))),300])\n",
        "  for i, j in enumerate(a):\n",
        "    if(len(j)==0):\n",
        "      continue\n",
        "    b[i][0:len(j)] = j\n",
        "    b[i][len(j):] = np.zeros(300,)\n",
        "  return b\n",
        "\n",
        "\n",
        "df['vectors'] = df['vectors'].apply(lambda x: np.array(list(filter(lambda y: y.ndim > 1, x))))\n",
        "df = df[df.vectors.map(lambda d: len(d)) >0].reset_index().drop('index',axis=1)\n",
        "df['vectors'] = df['vectors'].apply(lambda x: np.array(list(map(lambda y: y.reshape(300,), np.concatenate(x)))))\n",
        "vectorList = paddingEmbedded(df['vectors'].array)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## NN ALGORITHMS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "training_size = 0.8\n",
        "\n",
        "sentences = []\n",
        "labels = []\n",
        "\n",
        "for ind in df.index:\n",
        "    sentences.append(vectorList[ind])\n",
        "    labels.append(df['type'][ind])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "training_sentences, testing_sentences, training_labels, testing_labels = train_test_split(sentences, labels, test_size=1-training_size, random_state=42, stratify=labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Need this block to get it to work with TensorFlow 2.x\n",
        "import numpy as np\n",
        "training_labels = np.array(training_labels)\n",
        "testing_labels = np.array(testing_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "encoder = OneHotEncoder()\n",
        "training_labels = encoder.fit_transform(training_labels.reshape(-1,1)).toarray()\n",
        "testing_labels = encoder.fit_transform(testing_labels.reshape(-1,1)).toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# testing_sentences = np.array(list(testing_sentences))\n",
        "# training_sentences = np.array(list(training_sentences))\n",
        "testing_sentences = tf.convert_to_tensor(testing_sentences)\n",
        "training_sentences = tf.convert_to_tensor(training_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "classes = 4\n",
        "\n",
        "filters = 32\n",
        "pool_size = 2\n",
        "kernel_size = 3\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dropout(0.8),\n",
        "    tf.keras.layers.Conv1D(filters, kernel_size, padding='valid',activation='relu'),\n",
        "    tf.keras.layers.MaxPooling1D(pool_size),\n",
        "    tf.keras.layers.LSTM(120,return_sequences=True),\n",
        "    tf.keras.layers.GlobalAveragePooling1D(),\n",
        "    tf.keras.layers.Dense(classes,activation='softmax')\n",
        "])\n",
        "model.compile(loss=tf.keras.losses.CategoricalCrossentropy(),optimizer=tf.keras.optimizers.Adam(),metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "NEpoch = 32\n",
        "batch_size = 32\n",
        "history = model.fit(training_sentences, training_labels, batch_size=batch_size, epochs=NEpoch, validation_data=(testing_sentences, testing_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.save('saved_models/450Set_spacy')\n",
        "model.save('saved_models/450Set_spacy.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.plot(history.history['val_'+string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.legend([string, 'val_'+string])\n",
        "  plt.show()\n",
        "  \n",
        "plot_graphs(history, \"accuracy\")\n",
        "plot_graphs(history, \"loss\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def paddingEmbedded(a):\n",
        "  b = np.zeros([len(a), len(max(a, key=lambda x: len(x))), 300])\n",
        "  for i, j in enumerate(a):\n",
        "    if (len(j) == 0):\n",
        "      continue\n",
        "    b[i][0:len(j)] = j\n",
        "    b[i][len(j):] = np.zeros(300,)\n",
        "  return b\n",
        "\n",
        "def word2vec(post):\n",
        "  vecList = []\n",
        "  # post = list(map(lambda x: x.split(),post))\n",
        "  docs = nlp(post)\n",
        "  for doc in docs:\n",
        "    vecList.append(doc.vector)\n",
        "\n",
        "  vecList=np.array(vecList)\n",
        "  return np.array(vecList)\n",
        "\n",
        "\n",
        "\n",
        "sentence = [\"I fully believe power protector voice voiceless so spirit I present film hope recieve spirit compassion om mani padme hum yes right but time help come outside relationshippartner supportive helpful I think get right help critical finding good if do not feel worthy love difference I think jawz I think indicative deep issue long meet it mean friend lookingglass thank make point unfortunately corrupt christianity speak loud voice still small voice forget very christian today this curse I get to tell people prefer remain stuck neurosis excuse of course spite scar I ve I know painful callie honestly I think dodged bullet I m sure he s good person sound emotional mature being involve you guy singe song I ve research heuristic mental shortcuts cognitive bias explain lot people for principle I empathize sentiment I think truly religion contain need society easily manipulate control this dualistic I think forget type make person emotional social competency past hurt issue value meme etc I see experiment crowd I do not feel rare I feel thank uk sound like reasonable compassionate person I think neitzsche quote beware fight monster lest process appropriate here colorado it surprise learn cognitive brain science discover human actually hard - wire empathy read golemans social intelligence help understand you understand aegis blame yes like inhuman monster try justify marginalize human people like do not understand you know idea riddle fallacy linquistic manipulation is not worth time continue conversation hopefully year pass learn think an elephant swallow boa constrictor love book I cry little end thoughtful philosophy sure I like line the little prince signature block I use accord logic tolerance accept intolerance consider tolerant homosexuality violent behavior conflate extremely okay guy record I think healthy debate help forward simple heuristic accompany cognitive bias prevalent religion critical thinking look like explain I m sorry I do not notice post do not mean ignore the teleological argument definitely major flaw conclusion draw okay attribute else quote second problem this post logical fallacy introduce straw - man no problem look light moment re - frame argument no introduce idea tolerant clever use slippery slope do not blame touch gay marriage good I far faith reason inherently contradictory I word post way I you understand I spend life study religion no suppose know inner council can not people aware communicate telepathically year disguise regular folk so get decide good and I would not word like logic religion concern little religion stand critical inquiry old school infj theme song veteran psychic war blue oyster cult okay world bullshit I m go to change frame reference up point I ve operate think world vampire define get olderfor life I try squeeze role I think I need order I think people know like you plate pardner pretty typical ill bet stuff go inside reasonably articulate this I love I like card do not write creative after argument wife I find little dog I write run away love thread over year I practice religion philosophy I think infj mean explorer extent during phase exploration I practice I think I hate crap meaningless usually people feel good the truth people self - center do not understand love I understand it bother lot I walk store feel angry judgmental add sap energy feeling I do not like the great sage linus van pelt say I love mankind people I can not stand I find end individually I love people group funny come long way sometimes I try talk like oh man do not know sure chance come accord cognitive brain science truly tune human synchrony brain wave with discovery mirror neuron look like stumble buddhist first noble truth okay I m go to sound lame go you go find keep change grow nature ever try friend stray cat the rule apply do not chase run provide safe friendly harbor watch quickly affectionate well like someones post thank un thank thank oh chazz that feeling come go I would not mean ever I remember I feel like I belong as child I impression presence okay totally agree post I could not help get visual intimidate typist I see roz monsters inc it take little time tell difference anxious perception clear intuition that say year I manage screw royally time great movie I think movie move wing of desire this film old school style direct wim wender feature talent I m sure experience maybe I m middle aged I thing way I musician artist salesman soldier nurse cna technician christian buddhist I find mirror neuron read daniel golemans social intelligence credit research go italian research scientist escape ai not funive reading mirror neuron genuinely engage human brain actually sync now have compile information link mb type soma type I wonder expectation little unrealistic time sometimes I feel emotionally raw wound need withdraw oddly time like little\",\n",
        "            \"I really enjoyed this anime, but some of the characters were a bit annoying. Also, it would have been a lot better if the ending wasn't so horrible.\",\n",
        "            \"From time to time, I have the urge to punch a random stranger on the street. My intrusive thoughts are hard to control, and one time, when I couldn't win in a fight against them, I ended up in prison. I still don't remember what i did. The rumor has it that I killed them.\"]\n",
        "embedded = []\n",
        "for post in sentence:\n",
        "  embedded.append(word2vec(post))\n",
        "\n",
        "embedded = np.array(embedded)\n",
        "\n",
        "embedded = paddingEmbedded(embedded)\n",
        "\n",
        "print(model.predict(embedded))\n",
        "## correct is\n",
        "## [2]\n",
        "## [0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(max(history.history['val_accuracy']))\n",
        "print(min(history.history['val_loss']))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
        "import seaborn as sns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predict = model.predict(testing_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from math import isnan\n",
        "################ CONFUSION MATRIX ####################\n",
        "# Get and reshape confusion matrix data\n",
        "matrix = confusion_matrix(np.argmax(predict,axis=1), np.argmax(testing_labels,axis=1))\n",
        "matrix = matrix.astype('float') / matrix.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "\n",
        "for index1, i in enumerate(matrix):\n",
        "  for index2, j in enumerate(i):\n",
        "    if isnan(j):\n",
        "      matrix[index1][index2] = 0;\n",
        "\n",
        "# Build the plot\n",
        "plt.figure(figsize=(16, 7))\n",
        "sns.set(font_scale=1.4)\n",
        "sns.heatmap(matrix, annot=True, annot_kws={'size': 10},\n",
        "            cmap=plt.cm.Greens, linewidths=0.2)\n",
        "\n",
        "# Add labels to the plot\n",
        "class_names = ['GUARDIAN', 'ARTISAN', 'IDEALIST', 'RATIONALIST']\n",
        "tick_marks = np.arange(len(class_names))\n",
        "tick_marks2 = tick_marks + 0.5\n",
        "plt.xticks(tick_marks, class_names, rotation=25)\n",
        "plt.yticks(tick_marks2, class_names, rotation=0)\n",
        "plt.xlabel('Predicted label')\n",
        "plt.ylabel('True label')\n",
        "plt.title('Confusion Matrix for Random Forest Model')\n",
        "plt.show()\n",
        "################ CONFUSION MATRIX ####################\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sklearn.metrics as skm\n",
        "\n",
        "pred_max = np.argmax(predict,axis=1)\n",
        "test_max = np.argmax(testing_labels,axis=1)\n",
        "\n",
        "print(skm.classification_report(test_max,pred_max))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPipjDsvaeWPmm+I2AKTUv7",
      "collapsed_sections": [],
      "mount_file_id": "1_TEjG8vJVKTW0yXQp99YYob3Mif_LgsQ",
      "name": "NLPproject.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
