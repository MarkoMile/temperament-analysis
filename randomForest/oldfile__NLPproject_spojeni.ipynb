{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "executionInfo": {
          "elapsed": 14170,
          "status": "ok",
          "timestamp": 1660489158953,
          "user": {
            "displayName": "Marko Milenković",
            "userId": "00943616786887211077"
          },
          "user_tz": -120
        },
        "id": "vw4zrN3dhVxh"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/marko/.local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "2022-09-24 22:57:01.123290: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
            "2022-09-24 22:57:01.123309: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
            "2022-09-24 22:57:02.316487: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
            "2022-09-24 22:57:02.316504: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
            "2022-09-24 22:57:02.316521: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (marko-pc): /proc/driver/nvidia/version does not exist\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "import re\n",
        "import tldextract\n",
        "# import tensorflow as tf\n",
        "import numpy as np\n",
        "import unicodedata\n",
        "import os\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "STOP_WORDS = nlp.Defaults.stop_words\n",
        "\n",
        "nRowsRead = None  # specify 'None' if want to read whole file\n",
        "postStrings = []\n",
        "typeStrings = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "executionInfo": {
          "elapsed": 409,
          "status": "ok",
          "timestamp": 1660489731537,
          "user": {
            "displayName": "Marko Milenković",
            "userId": "00943616786887211077"
          },
          "user_tz": -120
        },
        "id": "YmgbW9Ax3QmE"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('./input/mbti_1.csv', delimiter=',', nrows=nRowsRead)\n",
        "df.dataframeName = 'mbti_1.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "executionInfo": {
          "elapsed": 4,
          "status": "ok",
          "timestamp": 1660489735283,
          "user": {
            "displayName": "Marko Milenković",
            "userId": "00943616786887211077"
          },
          "user_tz": -120
        },
        "id": "me0evtXa3ScU"
      },
      "outputs": [],
      "source": [
        "# making array of array of posts\n",
        "for person,type in zip(df['posts'],df['type']):\n",
        "    postStrings.append([type,person.split('|||')])\n",
        "\n",
        "# removing excess single parentheses\n",
        "\n",
        "for i in range(0,len(postStrings)):\n",
        "    postStrings[i][1][0] = postStrings[i][1][0][1:]\n",
        "    postStrings[i][1][-1] = postStrings[i][1][-1][:-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>type</th>\n",
              "      <th>posts</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>INFJ</td>\n",
              "      <td>[http://www.youtube.com/watch?v=qsXHcwe3krw, h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ENTP</td>\n",
              "      <td>[I'm finding the lack of me in these posts ver...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>INTP</td>\n",
              "      <td>[Good one  _____   https://www.youtube.com/wat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>INTJ</td>\n",
              "      <td>[Dear INTP,   I enjoyed our conversation the o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ENTJ</td>\n",
              "      <td>[You're fired., That's another silly misconcep...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   type                                              posts\n",
              "0  INFJ  [http://www.youtube.com/watch?v=qsXHcwe3krw, h...\n",
              "1  ENTP  [I'm finding the lack of me in these posts ver...\n",
              "2  INTP  [Good one  _____   https://www.youtube.com/wat...\n",
              "3  INTJ  [Dear INTP,   I enjoyed our conversation the o...\n",
              "4  ENTJ  [You're fired., That's another silly misconcep..."
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#splitting posts into list of posts\n",
        "df['posts'] = df['posts'].apply(lambda x: x.split('|||'))\n",
        "\n",
        "# removing excess single parentheses\n",
        "def removeParentheses(posts):\n",
        "    posts[0] = posts[0][1:]\n",
        "    posts[-1] = posts[-1][:-1]\n",
        "    return posts\n",
        "df['posts'] = df['posts'].apply(lambda x: removeParentheses(x))\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>type</th>\n",
              "      <th>posts</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>[http://www.youtube.com/watch?v=qsXHcwe3krw, h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>[I'm finding the lack of me in these posts ver...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>[Good one  _____   https://www.youtube.com/wat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>[Dear INTP,   I enjoyed our conversation the o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>[You're fired., That's another silly misconcep...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   type                                              posts\n",
              "0     2  [http://www.youtube.com/watch?v=qsXHcwe3krw, h...\n",
              "1     3  [I'm finding the lack of me in these posts ver...\n",
              "2     3  [Good one  _____   https://www.youtube.com/wat...\n",
              "3     3  [Dear INTP,   I enjoyed our conversation the o...\n",
              "4     3  [You're fired., That's another silly misconcep..."
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#label encoding type\n",
        "\n",
        "def labelEncodeType(type):\n",
        "  if type == 'ISTJ' or type == 'ISFJ' or type == 'ESTJ' or type == 'ESFJ':\n",
        "    type = 0 #GUARDIAN\n",
        "  elif type == 'ISTP' or type == 'ISFP' or type == 'ESTP' or type == 'ESFP':\n",
        "    type = 1 #ARTISAN\n",
        "  elif type == 'INFJ' or type == 'INFP' or type == 'ENFP' or type == 'ENFJ':\n",
        "    type = 2  # IDEALIST\n",
        "  elif type == 'INTJ' or type == 'INTP' or type == 'ENTP' or type == 'ENTJ':\n",
        "    type = 3  # RATIONALIST\n",
        "  return type\n",
        "\n",
        "df['type'] = df['type'].apply(lambda x: labelEncodeType(x))\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2    4167\n",
              "3    3311\n",
              "1     745\n",
              "0     452\n",
              "Name: type, dtype: int64"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['type'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**PREPROCESSING PIPELINE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "URL_REGEX = r\"\"\"(?i)\\b((?:[a-z][\\w-]+:(?:/{1,3}|[a-z0-9%])|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))\"\"\"\n",
        "DOMAIN_REGEX = \"^(?:https?:\\/\\/)?(?:[^@\\/\\n]+@)?(?:www\\.)?([^:\\/?\\n]+)\"\n",
        "COMMENT_REGEX = r\"\\[.*?\\]\"\n",
        "ELLIPSIS_REGEX = r\"^(\\ *\\.{3}\\ *)|(\\ *\\.{3}\\ *)$\"\n",
        "\n",
        "def urlReplace(stringToReplace):\n",
        "\tdomain = tldextract.extract(stringToReplace.group())\n",
        "\treturn domain.domain + '.' + domain.suffix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>type</th>\n",
              "      <th>posts</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>[enfp and intj moments youtube.com sportscente...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>[I'm finding the lack of me in these posts ver...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>[Good one _____ https://www.youtube.com/watch?...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>[Dear INTP, I enjoyed our conversation the oth...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>[You're fired., That's another silly misconcep...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   type                                              posts\n",
              "0     2  [enfp and intj moments youtube.com sportscente...\n",
              "1     3  [I'm finding the lack of me in these posts ver...\n",
              "2     3  [Good one _____ https://www.youtube.com/watch?...\n",
              "3     3  [Dear INTP, I enjoyed our conversation the oth...\n",
              "4     3  [You're fired., That's another silly misconcep..."
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def containsOnlyUrlsOrNumbers(post):\n",
        "\tsplits = post.split()\n",
        "\tfor str in splits:\n",
        "\t\tif(not re.search(URL_REGEX,str) and not re.search('^\\d+$',str) and not re.search('^\\.\\.\\.+$',str)):\n",
        "\t\t\treturn 1\n",
        "\treturn 0\n",
        "\n",
        "def containsComments(post):\n",
        "\tsplits = post.split('] ')\n",
        "\tfor str in splits:\n",
        "\t\tif(not re.search(COMMENT_REGEX,str)):\n",
        "\t\t\treturn 1\n",
        "\t\treturn 0\n",
        "\n",
        "def removeEllipsis(post):\n",
        "\treturn re.sub(ELLIPSIS_REGEX,'',post)\n",
        "\n",
        "def removeWhitespace(post):\n",
        "\tpost = \" \".join(post.split())\n",
        "\treturn post\n",
        "\n",
        "def removeAccentedChars(post):\n",
        "\tpost = unicodedata.normalize('NFKD',post).encode('ascii','ignore').decode('utf-8','ignore')\n",
        "\treturn post\n",
        "\n",
        "def formatPerson(person):\n",
        "\tperson = list(map(lambda x: removeWhitespace(x), person))\n",
        "\tperson = [post for post in person if (containsOnlyUrlsOrNumbers(post) and containsComments(post))]\n",
        "\tperson = list(map(lambda x: removeEllipsis(x), person))\n",
        "\tperson = list(map(lambda x: removeAccentedChars(x), person))\n",
        "\treturn person\n",
        "\n",
        "df['posts']= df['posts'].apply(lambda x:formatPerson(x))\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**ADDITIONAL FEATURES AND FINAL FORMATTING**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/home/marko/PETNICA/temperament-analysis/randomForest/NLPproject_spojeni.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 62>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/marko/PETNICA/temperament-analysis/randomForest/NLPproject_spojeni.ipynb#X13sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mstopWordCount\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mposts\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x:\u001b[39msum\u001b[39m(\u001b[39mmap\u001b[39m(\u001b[39mlambda\u001b[39;00m y: \u001b[39mlen\u001b[39m([t \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m y\u001b[39m.\u001b[39msplit() \u001b[39mif\u001b[39;00m t \u001b[39min\u001b[39;00m STOP_WORDS]),x)))  \u001b[39m#COUNT STOPWORDS\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/marko/PETNICA/temperament-analysis/randomForest/NLPproject_spojeni.ipynb#X13sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39murlCount\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mposts\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x:\u001b[39msum\u001b[39m(\u001b[39mmap\u001b[39m(\u001b[39mlambda\u001b[39;00m y:countUrls(\u001b[39mstr\u001b[39m(y)),x)))   \u001b[39m#COUNT URLS\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/marko/PETNICA/temperament-analysis/randomForest/NLPproject_spojeni.ipynb#X13sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mposts\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39;49m\u001b[39mposts\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(\u001b[39mlambda\u001b[39;49;00m x:\u001b[39mlist\u001b[39;49m(\u001b[39mmap\u001b[39;49m(\u001b[39mlambda\u001b[39;49;00m y: re\u001b[39m.\u001b[39;49msub(URL_REGEX, urlReplace, y), x))) \u001b[39m#remove urls\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/marko/PETNICA/temperament-analysis/randomForest/NLPproject_spojeni.ipynb#X13sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mwordCount\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mposts\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x:\u001b[39msum\u001b[39m(\u001b[39mmap\u001b[39m(\u001b[39mlambda\u001b[39;00m y:countWords(\u001b[39mstr\u001b[39m(y)),x)))         \u001b[39m#COUNT WORDS\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/marko/PETNICA/temperament-analysis/randomForest/NLPproject_spojeni.ipynb#X13sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mavgWordLen\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mposts\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x:\u001b[39msum\u001b[39m(\u001b[39mmap\u001b[39m(\u001b[39mlambda\u001b[39;00m y:avgWordLen(\u001b[39mstr\u001b[39m(y)),x))\u001b[39m/\u001b[39m\u001b[39mlen\u001b[39m(x)) \u001b[39m#AVERAGE WORD LEN\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/series.py:4433\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4323\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[1;32m   4324\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   4325\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4328\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   4329\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[1;32m   4330\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4331\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4332\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4431\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4432\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4433\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/apply.py:1088\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1084\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf, \u001b[39mstr\u001b[39m):\n\u001b[1;32m   1085\u001b[0m     \u001b[39m# if we are a string, try to dispatch\u001b[39;00m\n\u001b[1;32m   1086\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[0;32m-> 1088\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/apply.py:1143\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1137\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[1;32m   1138\u001b[0m         \u001b[39m# error: Argument 2 to \"map_infer\" has incompatible type\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m         \u001b[39m# \"Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],\u001b[39;00m\n\u001b[1;32m   1140\u001b[0m         \u001b[39m# Dict[Hashable, Union[Union[Callable[..., Any], str],\u001b[39;00m\n\u001b[1;32m   1141\u001b[0m         \u001b[39m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[39;00m\n\u001b[1;32m   1142\u001b[0m         \u001b[39m# \"Callable[[Any], Any]\"\u001b[39;00m\n\u001b[0;32m-> 1143\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[1;32m   1144\u001b[0m             values,\n\u001b[1;32m   1145\u001b[0m             f,  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1146\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[1;32m   1147\u001b[0m         )\n\u001b[1;32m   1149\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1150\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1151\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1152\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/_libs/lib.pyx:2870\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[1;32m/home/marko/PETNICA/temperament-analysis/randomForest/NLPproject_spojeni.ipynb Cell 11\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/marko/PETNICA/temperament-analysis/randomForest/NLPproject_spojeni.ipynb#X13sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mstopWordCount\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mposts\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x:\u001b[39msum\u001b[39m(\u001b[39mmap\u001b[39m(\u001b[39mlambda\u001b[39;00m y: \u001b[39mlen\u001b[39m([t \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m y\u001b[39m.\u001b[39msplit() \u001b[39mif\u001b[39;00m t \u001b[39min\u001b[39;00m STOP_WORDS]),x)))  \u001b[39m#COUNT STOPWORDS\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/marko/PETNICA/temperament-analysis/randomForest/NLPproject_spojeni.ipynb#X13sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39murlCount\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mposts\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x:\u001b[39msum\u001b[39m(\u001b[39mmap\u001b[39m(\u001b[39mlambda\u001b[39;00m y:countUrls(\u001b[39mstr\u001b[39m(y)),x)))   \u001b[39m#COUNT URLS\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/marko/PETNICA/temperament-analysis/randomForest/NLPproject_spojeni.ipynb#X13sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mposts\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mposts\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x:\u001b[39mlist\u001b[39;49m(\u001b[39mmap\u001b[39;49m(\u001b[39mlambda\u001b[39;49;00m y: re\u001b[39m.\u001b[39;49msub(URL_REGEX, urlReplace, y), x))) \u001b[39m#remove urls\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/marko/PETNICA/temperament-analysis/randomForest/NLPproject_spojeni.ipynb#X13sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mwordCount\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mposts\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x:\u001b[39msum\u001b[39m(\u001b[39mmap\u001b[39m(\u001b[39mlambda\u001b[39;00m y:countWords(\u001b[39mstr\u001b[39m(y)),x)))         \u001b[39m#COUNT WORDS\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/marko/PETNICA/temperament-analysis/randomForest/NLPproject_spojeni.ipynb#X13sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mavgWordLen\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mposts\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x:\u001b[39msum\u001b[39m(\u001b[39mmap\u001b[39m(\u001b[39mlambda\u001b[39;00m y:avgWordLen(\u001b[39mstr\u001b[39m(y)),x))\u001b[39m/\u001b[39m\u001b[39mlen\u001b[39m(x)) \u001b[39m#AVERAGE WORD LEN\u001b[39;00m\n",
            "\u001b[1;32m/home/marko/PETNICA/temperament-analysis/randomForest/NLPproject_spojeni.ipynb Cell 11\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/marko/PETNICA/temperament-analysis/randomForest/NLPproject_spojeni.ipynb#X13sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mstopWordCount\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mposts\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x:\u001b[39msum\u001b[39m(\u001b[39mmap\u001b[39m(\u001b[39mlambda\u001b[39;00m y: \u001b[39mlen\u001b[39m([t \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m y\u001b[39m.\u001b[39msplit() \u001b[39mif\u001b[39;00m t \u001b[39min\u001b[39;00m STOP_WORDS]),x)))  \u001b[39m#COUNT STOPWORDS\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/marko/PETNICA/temperament-analysis/randomForest/NLPproject_spojeni.ipynb#X13sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39murlCount\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mposts\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x:\u001b[39msum\u001b[39m(\u001b[39mmap\u001b[39m(\u001b[39mlambda\u001b[39;00m y:countUrls(\u001b[39mstr\u001b[39m(y)),x)))   \u001b[39m#COUNT URLS\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/marko/PETNICA/temperament-analysis/randomForest/NLPproject_spojeni.ipynb#X13sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mposts\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mposts\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x:\u001b[39mlist\u001b[39m(\u001b[39mmap\u001b[39m(\u001b[39mlambda\u001b[39;00m y: re\u001b[39m.\u001b[39;49msub(URL_REGEX, urlReplace, y), x))) \u001b[39m#remove urls\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/marko/PETNICA/temperament-analysis/randomForest/NLPproject_spojeni.ipynb#X13sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mwordCount\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mposts\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x:\u001b[39msum\u001b[39m(\u001b[39mmap\u001b[39m(\u001b[39mlambda\u001b[39;00m y:countWords(\u001b[39mstr\u001b[39m(y)),x)))         \u001b[39m#COUNT WORDS\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/marko/PETNICA/temperament-analysis/randomForest/NLPproject_spojeni.ipynb#X13sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mavgWordLen\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mposts\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x:\u001b[39msum\u001b[39m(\u001b[39mmap\u001b[39m(\u001b[39mlambda\u001b[39;00m y:avgWordLen(\u001b[39mstr\u001b[39m(y)),x))\u001b[39m/\u001b[39m\u001b[39mlen\u001b[39m(x)) \u001b[39m#AVERAGE WORD LEN\u001b[39;00m\n",
            "File \u001b[0;32m/usr/lib64/python3.10/re.py:209\u001b[0m, in \u001b[0;36msub\u001b[0;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msub\u001b[39m(pattern, repl, string, count\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, flags\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[39m\"\"\"Return the string obtained by replacing the leftmost\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[39m    non-overlapping occurrences of the pattern in string by the\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[39m    replacement repl.  repl can be either a string or a callable;\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[39m    if a string, backslash escapes in it are processed.  If it is\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[39m    a callable, it's passed the Match object and must return\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[39m    a replacement string to be used.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 209\u001b[0m     \u001b[39mreturn\u001b[39;00m _compile(pattern, flags)\u001b[39m.\u001b[39;49msub(repl, string, count)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "def toLower(post):\n",
        "\tpost = post.lower()\n",
        "\treturn post\n",
        "\n",
        "def countUpper(post):\n",
        "  upperCount = sum(map(str.isupper, post.split()))\n",
        "  return upperCount\n",
        "\n",
        "def countWords(post):\n",
        "  count = len(post.split())\n",
        "  return count\n",
        "\n",
        "def avgWordLen(post):\n",
        "  words = post.split()\n",
        "  wordLen = 0\n",
        "  for word in words:\n",
        "    wordLen = wordLen + len(word)\n",
        "  if len(words)>0:\n",
        "    return wordLen/len(words)\n",
        "  else: \n",
        "    return 0\n",
        "\n",
        "def countUrls(post):\n",
        "  return len(re.findall(URL_REGEX,post))\n",
        "\n",
        "def removeStopWords(post):\n",
        "\tpost = \" \".join([t for t in post.split() if t not in STOP_WORDS])\n",
        "\treturn post\n",
        "\n",
        "def removeSpecialChars(post): #ALSO REMOVES PUNCTUATION\n",
        "\tpost = re.sub('[^A-Z a-z 0-9-]+','',post)\n",
        "\treturn post\n",
        "\n",
        "def removeAlphaNumeric(post):\n",
        "  alphaNumRegex = r\"\\b([A-z]+[0-9]+[A-z0-9]*|[0-9]+[A-z]+[A-z0-9]*)\\b\"\n",
        "  post = re.sub(alphaNumRegex, '', post)\n",
        "  return post\n",
        "\n",
        "\n",
        "def removeNumbers(post):\n",
        "  numberRegex = r\"[0-9]+\"\n",
        "  floatRegex = r\"([1-9][0-9]*[eE][1-9][0-9]*|(([1-9][0-9]*\\.)|(\\.[0-9]+))([0-9]*)?([eE][\\-\\+]?[1-9][0-9]*)?)\"\n",
        "  post = re.sub(floatRegex, '', post)\n",
        "  post = re.sub(numberRegex, '', post)\n",
        "  return post\n",
        "\n",
        "def shortenRepeatingChars(post):\n",
        "  repeatingCharRegex = r\"(.)\\1{3,}\" #finds character groups of more than 3 consecutive chars\n",
        "  post = re.sub(repeatingCharRegex,r\"\\1\\1\\1\",post)\n",
        "  return post\n",
        "\n",
        "def removeSingleChars(post): #removes free single chars EXCEPT I or i\n",
        "  singleCharRegex = r\"(^| )[^Ii](( )[^Ii])*( |$)\"\n",
        "  post = re.sub(singleCharRegex,'',post)\n",
        "  return post\n",
        "  \n",
        "\n",
        "df['upperCount'] = df['posts'].apply(lambda x:sum(map(lambda y:countUpper(str(y)),x)))  #COUNT UPPERCASE WORDS\n",
        "df['stopWordCount'] = df['posts'].apply(lambda x:sum(map(lambda y: len([t for t in y.split() if t in STOP_WORDS]),x)))  #COUNT STOPWORDS\n",
        "df['urlCount'] = df['posts'].apply(lambda x:sum(map(lambda y:countUrls(str(y)),x)))   #COUNT URLS\n",
        "\n",
        "df['posts'] = df['posts'].apply(lambda x:list(map(lambda y: re.sub(URL_REGEX, urlReplace, y), x))) #remove urls\n",
        "\n",
        "df['wordCount'] = df['posts'].apply(lambda x:sum(map(lambda y:countWords(str(y)),x)))         #COUNT WORDS\n",
        "df['avgWordLen'] = df['posts'].apply(lambda x:sum(map(lambda y:avgWordLen(str(y)),x))/len(x)) #AVERAGE WORD LEN\n",
        "\n",
        "df['posts'] = df['posts'].apply(lambda x:list(map(lambda y: removeSpecialChars(str(y)), x))) #remove special chars (including punctuation)\n",
        "df['posts'] = df['posts'].apply(lambda x:list(map(lambda y:removeStopWords(str(y)),x))) #remove stopwords\n",
        "df['posts'] = df['posts'].apply(lambda x:list(map(lambda y: removeAlphaNumeric(str(y)), x)))  #remove alphanumeric words\n",
        "df['posts'] = df['posts'].apply(lambda x:list(map(lambda y: removeNumbers(str(y)), x))) #remove numbers (int and float)\n",
        "df['posts'] = df['posts'].apply(lambda x:list(map(lambda y:toLower(str(y)),x))) #lowercase all\n",
        "df['posts'] = df['posts'].apply(lambda x:list(map(lambda y: shortenRepeatingChars(str(y)), x))) #shorten all character repeats to 3 characters\n",
        "df['posts'] = df['posts'].apply(lambda x:list(map(lambda y: removeSingleChars(str(y)), x))) #remove all single character words except I and i\n",
        "df['posts'] = df['posts'].apply(lambda x:list(map(lambda y: removeWhitespace(str(y)), x))) #remove excess whitespaces again to clean up\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.to_csv(path_or_buf=\"./csv_files/df_spojeni_unlemmatized.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 1934,
          "status": "ok",
          "timestamp": 1660490778787,
          "user": {
            "displayName": "Marko Milenković",
            "userId": "00943616786887211077"
          },
          "user_tz": -120
        },
        "id": "4lDomPao3UXN"
      },
      "outputs": [],
      "source": [
        "#do the nlp tokenization with spaCy\n",
        "\n",
        "def convertToLemmas(posts):\n",
        "  docs = nlp.pipe(posts, n_process=-1, disable=[\"parser\"])\n",
        "  lemmaList = []\n",
        "  postList = []\n",
        "  for doc in docs:\n",
        "    for token in doc:\n",
        "      lemma = str(token.lemma_)\n",
        "      if lemma == '-PRON-' or lemma == 'be':\n",
        "        lemma = token.text\n",
        "      lemmaList.append(lemma)\n",
        "    postList.append(\" \".join(lemmaList))\n",
        "    lemmaList= []\n",
        "  return \" \".join(postList)\n",
        "\n",
        "#for the dataframe:\n",
        "df['posts']= df['posts'].apply(lambda x:convertToLemmas(x))\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'pd' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/home/marko/PETNICA/temperament-analysis/randomForest/NLPproject_spojeni.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/marko/PETNICA/temperament-analysis/randomForest/NLPproject_spojeni.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# df.to_csv(path_or_buf=\"./csv_files/df_spojeni.csv\")\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/marko/PETNICA/temperament-analysis/randomForest/NLPproject_spojeni.ipynb#X16sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# df = pd.read_csv(filepath_or_buffer=\"./csv_files/df_spojeni.csv\")\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/marko/PETNICA/temperament-analysis/randomForest/NLPproject_spojeni.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# df=df.drop([3559])\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/marko/PETNICA/temperament-analysis/randomForest/NLPproject_spojeni.ipynb#X16sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_pickle(\u001b[39m\"\u001b[39m\u001b[39m./pickle_files/df_odvojeni_word_vectors.pkl\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/marko/PETNICA/temperament-analysis/randomForest/NLPproject_spojeni.ipynb#X16sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mgroupby([\u001b[39m'\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mavgWordLen\u001b[39m\u001b[39m'\u001b[39m], sort\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,as_index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\u001b[39m.\u001b[39magg({\u001b[39m'\u001b[39m\u001b[39mposts\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin,\u001b[39m'\u001b[39m\u001b[39mupperCount\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m'\u001b[39m\u001b[39mfirst\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mstopWordCount\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m'\u001b[39m\u001b[39mfirst\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39murlCount\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m'\u001b[39m\u001b[39mfirst\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mwordCount\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m'\u001b[39m\u001b[39mfirst\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mavgWordLen\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m'\u001b[39m\u001b[39mfirst\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mvectors\u001b[39m\u001b[39m'\u001b[39m:\u001b[39mlambda\u001b[39;00m x:np\u001b[39m.\u001b[39marray(x)})\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ],
      "source": [
        "# df.to_csv(path_or_buf=\"./csv_files/df_spojeni.csv\")\n",
        "df = pd.read_csv(filepath_or_buffer=\"./csv_files/df_spojeni.csv\")\n",
        "df=df.drop([3559])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Wordcloud before word removal**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# wc = WordCloud(width=800, height=400).generate(' '.join(df['posts'].tolist()))\n",
        "# plt.imshow(wc)\n",
        "# plt.axis('off')\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**common words removal**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "freq_comm = pd.Series(' '.join(df['posts'].tolist()).split()).value_counts()\n",
        "f20= freq_comm[:1]\n",
        "f20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['posts'] = df['posts'].apply(lambda x: ' '.join([t for t in x.split() if t not in f20]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**rare words removal**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rare= freq_comm[freq_comm.values==1]\n",
        "rare"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['posts'] = df['posts'].apply(lambda x: ' '.join([t for t in x.split() if t not in rare]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Wordcloud after word removal**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# wc = WordCloud(width=800,height=400).generate(' '.join(df['posts'].apply(lambda x: str(x)).tolist()))\n",
        "# plt.imshow(wc)\n",
        "# plt.axis('off')\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>type</th>\n",
              "      <th>posts</th>\n",
              "      <th>upperCount</th>\n",
              "      <th>stopWordCount</th>\n",
              "      <th>urlCount</th>\n",
              "      <th>wordCount</th>\n",
              "      <th>avgWordLen</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3179</th>\n",
              "      <td>3179</td>\n",
              "      <td>0</td>\n",
              "      <td>aptain phillip happy nice I m currently read e...</td>\n",
              "      <td>10</td>\n",
              "      <td>178</td>\n",
              "      <td>0</td>\n",
              "      <td>391</td>\n",
              "      <td>4.534834</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>914</th>\n",
              "      <td>914</td>\n",
              "      <td>0</td>\n",
              "      <td>lol that s I figure mean I like way I do not s...</td>\n",
              "      <td>91</td>\n",
              "      <td>617</td>\n",
              "      <td>0</td>\n",
              "      <td>1382</td>\n",
              "      <td>4.214347</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1361</th>\n",
              "      <td>1361</td>\n",
              "      <td>0</td>\n",
              "      <td>believe balance important you want able enjoyg...</td>\n",
              "      <td>67</td>\n",
              "      <td>688</td>\n",
              "      <td>2</td>\n",
              "      <td>1473</td>\n",
              "      <td>4.566697</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1539</th>\n",
              "      <td>1539</td>\n",
              "      <td>0</td>\n",
              "      <td>do not know come I great love christmas grow p...</td>\n",
              "      <td>88</td>\n",
              "      <td>554</td>\n",
              "      <td>0</td>\n",
              "      <td>1279</td>\n",
              "      <td>4.978543</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7559</th>\n",
              "      <td>7559</td>\n",
              "      <td>0</td>\n",
              "      <td>to honest I do not know I truly capable want m...</td>\n",
              "      <td>119</td>\n",
              "      <td>860</td>\n",
              "      <td>1</td>\n",
              "      <td>1783</td>\n",
              "      <td>4.223365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7200</th>\n",
              "      <td>7200</td>\n",
              "      <td>3</td>\n",
              "      <td>can not draw there s youtubecom is not obvious...</td>\n",
              "      <td>40</td>\n",
              "      <td>467</td>\n",
              "      <td>2</td>\n",
              "      <td>1061</td>\n",
              "      <td>4.666140</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1848</th>\n",
              "      <td>1848</td>\n",
              "      <td>3</td>\n",
              "      <td>in time I leadership role I think I great lead...</td>\n",
              "      <td>117</td>\n",
              "      <td>689</td>\n",
              "      <td>0</td>\n",
              "      <td>1485</td>\n",
              "      <td>4.252287</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5417</th>\n",
              "      <td>5417</td>\n",
              "      <td>3</td>\n",
              "      <td>yes immediately say just attention detail pape...</td>\n",
              "      <td>68</td>\n",
              "      <td>723</td>\n",
              "      <td>0</td>\n",
              "      <td>1656</td>\n",
              "      <td>4.706761</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1136</th>\n",
              "      <td>1136</td>\n",
              "      <td>3</td>\n",
              "      <td>anti - anxiety medication day able function so...</td>\n",
              "      <td>29</td>\n",
              "      <td>168</td>\n",
              "      <td>0</td>\n",
              "      <td>438</td>\n",
              "      <td>4.569014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2125</th>\n",
              "      <td>2125</td>\n",
              "      <td>3</td>\n",
              "      <td>youtubeangelsarah maclachlan youtubecom youtub...</td>\n",
              "      <td>36</td>\n",
              "      <td>252</td>\n",
              "      <td>4</td>\n",
              "      <td>773</td>\n",
              "      <td>5.241507</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1800 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Unnamed: 0  type                                              posts  \\\n",
              "3179        3179     0  aptain phillip happy nice I m currently read e...   \n",
              "914          914     0  lol that s I figure mean I like way I do not s...   \n",
              "1361        1361     0  believe balance important you want able enjoyg...   \n",
              "1539        1539     0  do not know come I great love christmas grow p...   \n",
              "7559        7559     0  to honest I do not know I truly capable want m...   \n",
              "...          ...   ...                                                ...   \n",
              "7200        7200     3  can not draw there s youtubecom is not obvious...   \n",
              "1848        1848     3  in time I leadership role I think I great lead...   \n",
              "5417        5417     3  yes immediately say just attention detail pape...   \n",
              "1136        1136     3  anti - anxiety medication day able function so...   \n",
              "2125        2125     3  youtubeangelsarah maclachlan youtubecom youtub...   \n",
              "\n",
              "      upperCount  stopWordCount  urlCount  wordCount  avgWordLen  \n",
              "3179          10            178         0        391    4.534834  \n",
              "914           91            617         0       1382    4.214347  \n",
              "1361          67            688         2       1473    4.566697  \n",
              "1539          88            554         0       1279    4.978543  \n",
              "7559         119            860         1       1783    4.223365  \n",
              "...          ...            ...       ...        ...         ...  \n",
              "7200          40            467         2       1061    4.666140  \n",
              "1848         117            689         0       1485    4.252287  \n",
              "5417          68            723         0       1656    4.706761  \n",
              "1136          29            168         0        438    4.569014  \n",
              "2125          36            252         4        773    5.241507  \n",
              "\n",
              "[1800 rows x 8 columns]"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df0 = df[df['type'] == 0].sample(450)\n",
        "df1 = df[df['type'] == 1].sample(450)\n",
        "df2 = df[df['type'] == 2].sample(450)\n",
        "df3 = df[df['type'] == 3].sample(450)\n",
        "\n",
        "dfr = pd.concat([df0,df1,df2,df3],)\n",
        "dfr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# dfr.to_csv(path_or_buf=\"./csv_files/dfr_spojeni.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y = dfr['type']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cv = CountVectorizer(dtype=np.uint8)\n",
        "text_counts = cv.fit_transform(dfr['posts'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text_counts.toarray().shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dfr_bow = pd.DataFrame(text_counts.toarray(),columns=cv.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dfr_bow.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Num. of words: \" + str(len(list(dfr_bow.columns))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dfr_features = dfr.drop(labels=['posts','type'],axis=1).reset_index(drop=True)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**this reset_index should be double checked**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# dfr_bow.to_csv(path_or_buf=\"./csv_files/dfr_bow.csv\")\n",
        "# dfr.to_csv(path_or_buf=\"./csv_files/dfr.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ML ALGORITHMS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score,classification_report\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rfc = RandomForestClassifier(random_state=42,n_estimators=500,n_jobs=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "def classify(X,y):\n",
        "  scaler = MinMaxScaler(feature_range=(0,1))\n",
        "  X=scaler.fit_transform(X)\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X,y,test_size= 0.2,random_state=42,stratify=y)\n",
        "\n",
        "  rfc.fit(X_train,y_train)\n",
        "  y_pred = rfc.predict(X_test)\n",
        "  ac = accuracy_score(y_test,y_pred)\n",
        "\n",
        "  ################ CONFUSION MATRIX ####################\n",
        "  # Get and reshape confusion matrix data\n",
        "  matrix = confusion_matrix(y_test, y_pred)\n",
        "  matrix = matrix.astype('float') / matrix.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "  # Build the plot\n",
        "  plt.figure(figsize=(16, 7))\n",
        "  sns.set(font_scale=1.4)\n",
        "  sns.heatmap(matrix, annot=True, annot_kws={'size': 10},\n",
        "              cmap=plt.cm.Greens, linewidths=0.2)\n",
        "\n",
        "  # Add labels to the plot\n",
        "  class_names = ['GUARDIAN', 'ARTISAN', 'IDEALIST', 'RATIONALIST']\n",
        "  tick_marks = np.arange(len(class_names))\n",
        "  tick_marks2 = tick_marks + 0.5\n",
        "  plt.xticks(tick_marks, class_names, rotation=25)\n",
        "  plt.yticks(tick_marks2, class_names, rotation=0)\n",
        "  plt.xlabel('Predicted label')\n",
        "  plt.ylabel('True label')\n",
        "  plt.title('Confusion Matrix for Random Forest Model')\n",
        "  plt.show()\n",
        "  ################ CONFUSION MATRIX ####################\n",
        "\n",
        "  print(\"accuracy: \",ac)\n",
        "  print(classification_report(y_test, y_pred))\n",
        "\n",
        "  ############### FEATURE IMPORTANCE ###################\n",
        "  features = scaler.get_feature_names_out()\n",
        "  fi = rfc.feature_importances_\n",
        "  importance = {features[i]:fi[i] for i in range(0, len(features))}\n",
        "  wc = WordCloud(width=800, height=400).generate_from_frequencies(importance)\n",
        "  plt.imshow(wc)\n",
        "  plt.axis('off')\n",
        "  plt.show()\n",
        "  ############### FEATURE IMPORTANCE ###################\n",
        "\n",
        "classify(dfr_features.join(dfr_bow),y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### average accuracy on 100 forests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def classify(X,y):\n",
        "#   scaler = MinMaxScaler(feature_range=(0,1))\n",
        "#   X=scaler.fit_transform(X)\n",
        "\n",
        "#   X_train, X_test, y_train, y_test = train_test_split(X,y,test_size= 0.2,random_state=42,stratify=y)\n",
        "\n",
        "#   rfc.fit(X_train,y_train)\n",
        "#   y_pred = rfc.predict(X_test)\n",
        "#   ac = accuracy_score(y_test,y_pred)\n",
        "\n",
        "#   return ac\n",
        "\n",
        "# test = []\n",
        "\n",
        "# for i in range(0,100):\n",
        "#   df0 = df[df['type'] == 0].sample(450)\n",
        "#   df1 = df[df['type'] == 1].sample(450)\n",
        "#   df2 = df[df['type'] == 2].sample(450)\n",
        "#   df3 = df[df['type'] == 3].sample(450)\n",
        "#   dfr = pd.concat([df0, df1, df2, df3],)\n",
        "#   y = dfr['type']\n",
        "#   cv = CountVectorizer(dtype='b')\n",
        "#   text_counts = cv.fit_transform(dfr['posts'])\n",
        "#   dfr_bow = pd.DataFrame(text_counts.toarray(),columns=cv.get_feature_names_out())\n",
        "#   dfr_features = dfr.drop(labels=['posts','type'],axis=1).reset_index(drop=True)  \n",
        "\n",
        "#   test.append(classify(dfr_features.join(dfr_bow), y))\n",
        "\n",
        "# print(sum(test)/len(test))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPipjDsvaeWPmm+I2AKTUv7",
      "collapsed_sections": [],
      "mount_file_id": "1_TEjG8vJVKTW0yXQp99YYob3Mif_LgsQ",
      "name": "NLPproject.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
