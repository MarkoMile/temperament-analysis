{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 14170,
          "status": "ok",
          "timestamp": 1660489158953,
          "user": {
            "displayName": "Marko Milenković",
            "userId": "00943616786887211077"
          },
          "user_tz": -120
        },
        "id": "vw4zrN3dhVxh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "import re\n",
        "import tldextract\n",
        "# import tensorflow as tf\n",
        "import numpy as np\n",
        "import unicodedata\n",
        "import os\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "STOP_WORDS = nlp.Defaults.stop_words\n",
        "\n",
        "nRowsRead = None  # specify 'None' if want to read whole file\n",
        "postStrings = []\n",
        "typeStrings = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 409,
          "status": "ok",
          "timestamp": 1660489731537,
          "user": {
            "displayName": "Marko Milenković",
            "userId": "00943616786887211077"
          },
          "user_tz": -120
        },
        "id": "YmgbW9Ax3QmE"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('./input/mbti_1.csv', delimiter=',', nrows=nRowsRead)\n",
        "df.dataframeName = 'mbti_1.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 4,
          "status": "ok",
          "timestamp": 1660489735283,
          "user": {
            "displayName": "Marko Milenković",
            "userId": "00943616786887211077"
          },
          "user_tz": -120
        },
        "id": "me0evtXa3ScU"
      },
      "outputs": [],
      "source": [
        "# making array of array of posts\n",
        "for person,type in zip(df['posts'],df['type']):\n",
        "    postStrings.append([type,person.split('|||')])\n",
        "\n",
        "# removing excess single parentheses\n",
        "\n",
        "for i in range(0,len(postStrings)):\n",
        "    postStrings[i][1][0] = postStrings[i][1][0][1:]\n",
        "    postStrings[i][1][-1] = postStrings[i][1][-1][:-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#splitting posts into list of posts\n",
        "df['posts'] = df['posts'].apply(lambda x: x.split('|||'))\n",
        "\n",
        "# removing excess single parentheses\n",
        "def removeParentheses(posts):\n",
        "    posts[0] = posts[0][1:]\n",
        "    posts[-1] = posts[-1][:-1]\n",
        "    return posts\n",
        "df['posts'] = df['posts'].apply(lambda x: removeParentheses(x))\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#label encoding type\n",
        "\n",
        "def labelEncodeType(type):\n",
        "  if type == 'ISTJ' or type == 'ISFJ' or type == 'ESTJ' or type == 'ESFJ':\n",
        "    type = 0 #GUARDIAN\n",
        "  elif type == 'ISTP' or type == 'ISFP' or type == 'ESTP' or type == 'ESFP':\n",
        "    type = 1 #ARTISAN\n",
        "  elif type == 'INFJ' or type == 'INFP' or type == 'ENFP' or type == 'ENFJ':\n",
        "    type = 2  # IDEALIST\n",
        "  elif type == 'INTJ' or type == 'INTP' or type == 'ENTP' or type == 'ENTJ':\n",
        "    type = 3  # RATIONALIST\n",
        "  return type\n",
        "\n",
        "df['type'] = df['type'].apply(lambda x: labelEncodeType(x))\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['type'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**PREPROCESSING PIPELINE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "URL_REGEX = r\"\"\"(?i)\\b((?:[a-z][\\w-]+:(?:/{1,3}|[a-z0-9%])|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))\"\"\"\n",
        "DOMAIN_REGEX = \"^(?:https?:\\/\\/)?(?:[^@\\/\\n]+@)?(?:www\\.)?([^:\\/?\\n]+)\"\n",
        "COMMENT_REGEX = r\"\\[.*?\\]\"\n",
        "ELLIPSIS_REGEX = r\"^(\\ *\\.{3}\\ *)|(\\ *\\.{3}\\ *)$\"\n",
        "\n",
        "def urlReplace(stringToReplace):\n",
        "\tdomain = tldextract.extract(stringToReplace.group())\n",
        "\treturn domain.domain + '.' + domain.suffix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def containsOnlyUrlsOrNumbers(post):\n",
        "\tsplits = post.split()\n",
        "\tfor str in splits:\n",
        "\t\tif(not re.search(URL_REGEX,str) and not re.search('^\\d+$',str) and not re.search('^\\.\\.\\.+$',str)):\n",
        "\t\t\treturn 1\n",
        "\treturn 0\n",
        "\n",
        "def containsComments(post):\n",
        "\tsplits = post.split('] ')\n",
        "\tfor str in splits:\n",
        "\t\tif(not re.search(COMMENT_REGEX,str)):\n",
        "\t\t\treturn 1\n",
        "\t\treturn 0\n",
        "\n",
        "def removeEllipsis(post):\n",
        "\treturn re.sub(ELLIPSIS_REGEX,'',post)\n",
        "\n",
        "def removeWhitespace(post):\n",
        "\tpost = \" \".join(post.split())\n",
        "\treturn post\n",
        "\n",
        "def removeAccentedChars(post):\n",
        "\tpost = unicodedata.normalize('NFKD',post).encode('ascii','ignore').decode('utf-8','ignore')\n",
        "\treturn post\n",
        "\n",
        "def formatPerson(person):\n",
        "\tperson = list(map(lambda x: removeWhitespace(x), person))\n",
        "\tperson = [post for post in person if (containsOnlyUrlsOrNumbers(post) and containsComments(post))]\n",
        "\tperson = list(map(lambda x: removeEllipsis(x), person))\n",
        "\tperson = list(map(lambda x: removeAccentedChars(x), person))\n",
        "\treturn person\n",
        "\n",
        "df['posts']= df['posts'].apply(lambda x:formatPerson(x))\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**ADDITIONAL FEATURES AND FINAL FORMATTING**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def toLower(post):\n",
        "\tpost = post.lower()\n",
        "\treturn post\n",
        "\n",
        "def countUpper(post):\n",
        "  upperCount = sum(map(str.isupper, post.split()))\n",
        "  return upperCount\n",
        "\n",
        "def countWords(post):\n",
        "  count = len(post.split())\n",
        "  return count\n",
        "\n",
        "def avgWordLen(post):\n",
        "  words = post.split()\n",
        "  wordLen = 0\n",
        "  for word in words:\n",
        "    wordLen = wordLen + len(word)\n",
        "  if len(words)>0:\n",
        "    return wordLen/len(words)\n",
        "  else: \n",
        "    return 0\n",
        "\n",
        "def countUrls(post):\n",
        "  return len(re.findall(URL_REGEX,post))\n",
        "\n",
        "def removeStopWords(post):\n",
        "\tpost = \" \".join([t for t in post.split() if t not in STOP_WORDS])\n",
        "\treturn post\n",
        "\n",
        "def removeSpecialChars(post): #ALSO REMOVES PUNCTUATION\n",
        "\tpost = re.sub('[^A-Z a-z 0-9-]+','',post)\n",
        "\treturn post\n",
        "\n",
        "def removeAlphaNumeric(post):\n",
        "  alphaNumRegex = r\"\\b([A-z]+[0-9]+[A-z0-9]*|[0-9]+[A-z]+[A-z0-9]*)\\b\"\n",
        "  post = re.sub(alphaNumRegex, '', post)\n",
        "  return post\n",
        "\n",
        "\n",
        "def removeNumbers(post):\n",
        "  numberRegex = r\"[0-9]+\"\n",
        "  floatRegex = r\"([1-9][0-9]*[eE][1-9][0-9]*|(([1-9][0-9]*\\.)|(\\.[0-9]+))([0-9]*)?([eE][\\-\\+]?[1-9][0-9]*)?)\"\n",
        "  post = re.sub(floatRegex, '', post)\n",
        "  post = re.sub(numberRegex, '', post)\n",
        "  return post\n",
        "\n",
        "def shortenRepeatingChars(post):\n",
        "  repeatingCharRegex = r\"(.)\\1{3,}\" #finds character groups of more than 3 consecutive chars\n",
        "  post = re.sub(repeatingCharRegex,r\"\\1\\1\\1\",post)\n",
        "  return post\n",
        "\n",
        "def removeSingleChars(post): #removes free single chars EXCEPT I or i\n",
        "  singleCharRegex = r\"(^| )[^Ii](( )[^Ii])*( |$)\"\n",
        "  post = re.sub(singleCharRegex,'',post)\n",
        "  return post\n",
        "  \n",
        "\n",
        "df['upperCount'] = df['posts'].apply(lambda x:sum(map(lambda y:countUpper(str(y)),x)))  #COUNT UPPERCASE WORDS\n",
        "df['stopWordCount'] = df['posts'].apply(lambda x:sum(map(lambda y: len([t for t in y.split() if t in STOP_WORDS]),x)))  #COUNT STOPWORDS\n",
        "df['urlCount'] = df['posts'].apply(lambda x:sum(map(lambda y:countUrls(str(y)),x)))   #COUNT URLS\n",
        "\n",
        "df['posts'] = df['posts'].apply(lambda x:list(map(lambda y: re.sub(URL_REGEX, urlReplace, y), x))) #remove urls\n",
        "\n",
        "df['wordCount'] = df['posts'].apply(lambda x:sum(map(lambda y:countWords(str(y)),x)))         #COUNT WORDS\n",
        "df['avgWordLen'] = df['posts'].apply(lambda x:sum(map(lambda y:avgWordLen(str(y)),x))/len(x)) #AVERAGE WORD LEN\n",
        "\n",
        "df['posts'] = df['posts'].apply(lambda x:list(map(lambda y: removeSpecialChars(str(y)), x))) #remove special chars (including punctuation)\n",
        "df['posts'] = df['posts'].apply(lambda x:list(map(lambda y:removeStopWords(str(y)),x))) #remove stopwords\n",
        "df['posts'] = df['posts'].apply(lambda x:list(map(lambda y: removeAlphaNumeric(str(y)), x)))  #remove alphanumeric words\n",
        "df['posts'] = df['posts'].apply(lambda x:list(map(lambda y: removeNumbers(str(y)), x))) #remove numbers (int and float)\n",
        "df['posts'] = df['posts'].apply(lambda x:list(map(lambda y:toLower(str(y)),x))) #lowercase all\n",
        "df['posts'] = df['posts'].apply(lambda x:list(map(lambda y: shortenRepeatingChars(str(y)), x))) #shorten all character repeats to 3 characters\n",
        "df['posts'] = df['posts'].apply(lambda x:list(map(lambda y: removeSingleChars(str(y)), x))) #remove all single character words except I and i\n",
        "df['posts'] = df['posts'].apply(lambda x:list(map(lambda y: removeWhitespace(str(y)), x))) #remove excess whitespaces again to clean up\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.to_csv(path_or_buf=\"./csv_files/df_spojeni_unlemmatized.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 1934,
          "status": "ok",
          "timestamp": 1660490778787,
          "user": {
            "displayName": "Marko Milenković",
            "userId": "00943616786887211077"
          },
          "user_tz": -120
        },
        "id": "4lDomPao3UXN"
      },
      "outputs": [],
      "source": [
        "#do the nlp tokenization with spaCy\n",
        "\n",
        "def convertToLemmas(posts):\n",
        "  docs = nlp.pipe(posts, n_process=-1, disable=[\"parser\"])\n",
        "  lemmaList = []\n",
        "  postList = []\n",
        "  for doc in docs:\n",
        "    for token in doc:\n",
        "      lemma = str(token.lemma_)\n",
        "      if lemma == '-PRON-' or lemma == 'be':\n",
        "        lemma = token.text\n",
        "      lemmaList.append(lemma)\n",
        "    postList.append(\" \".join(lemmaList))\n",
        "    lemmaList= []\n",
        "  return \" \".join(postList)\n",
        "\n",
        "#for the dataframe:\n",
        "df['posts']= df['posts'].apply(lambda x:convertToLemmas(x))\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "# df.to_csv(path_or_buf=\"./csv_files/df_spojeni.csv\")\n",
        "df = pd.read_csv(filepath_or_buffer=\"./csv_files/df_spojeni.csv\")\n",
        "df=df.drop([3559])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Wordcloud before word removal**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# wc = WordCloud(width=800, height=400).generate(' '.join(df['posts'].tolist()))\n",
        "# plt.imshow(wc)\n",
        "# plt.axis('off')\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**common words removal**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "freq_comm = pd.Series(' '.join(df['posts'].tolist()).split()).value_counts()\n",
        "f20= freq_comm[:1]\n",
        "f20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['posts'] = df['posts'].apply(lambda x: ' '.join([t for t in x.split() if t not in f20]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**rare words removal**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rare= freq_comm[freq_comm.values==1]\n",
        "rare"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['posts'] = df['posts'].apply(lambda x: ' '.join([t for t in x.split() if t not in rare]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Wordcloud after word removal**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# wc = WordCloud(width=800,height=400).generate(' '.join(df['posts'].apply(lambda x: str(x)).tolist()))\n",
        "# plt.imshow(wc)\n",
        "# plt.axis('off')\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>type</th>\n",
              "      <th>posts</th>\n",
              "      <th>upperCount</th>\n",
              "      <th>stopWordCount</th>\n",
              "      <th>urlCount</th>\n",
              "      <th>wordCount</th>\n",
              "      <th>avgWordLen</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3179</th>\n",
              "      <td>3179</td>\n",
              "      <td>0</td>\n",
              "      <td>aptain phillip happy nice I m currently read e...</td>\n",
              "      <td>10</td>\n",
              "      <td>178</td>\n",
              "      <td>0</td>\n",
              "      <td>391</td>\n",
              "      <td>4.534834</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>914</th>\n",
              "      <td>914</td>\n",
              "      <td>0</td>\n",
              "      <td>lol that s I figure mean I like way I do not s...</td>\n",
              "      <td>91</td>\n",
              "      <td>617</td>\n",
              "      <td>0</td>\n",
              "      <td>1382</td>\n",
              "      <td>4.214347</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1361</th>\n",
              "      <td>1361</td>\n",
              "      <td>0</td>\n",
              "      <td>believe balance important you want able enjoyg...</td>\n",
              "      <td>67</td>\n",
              "      <td>688</td>\n",
              "      <td>2</td>\n",
              "      <td>1473</td>\n",
              "      <td>4.566697</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1539</th>\n",
              "      <td>1539</td>\n",
              "      <td>0</td>\n",
              "      <td>do not know come I great love christmas grow p...</td>\n",
              "      <td>88</td>\n",
              "      <td>554</td>\n",
              "      <td>0</td>\n",
              "      <td>1279</td>\n",
              "      <td>4.978543</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7559</th>\n",
              "      <td>7559</td>\n",
              "      <td>0</td>\n",
              "      <td>to honest I do not know I truly capable want m...</td>\n",
              "      <td>119</td>\n",
              "      <td>860</td>\n",
              "      <td>1</td>\n",
              "      <td>1783</td>\n",
              "      <td>4.223365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7200</th>\n",
              "      <td>7200</td>\n",
              "      <td>3</td>\n",
              "      <td>can not draw there s youtubecom is not obvious...</td>\n",
              "      <td>40</td>\n",
              "      <td>467</td>\n",
              "      <td>2</td>\n",
              "      <td>1061</td>\n",
              "      <td>4.666140</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1848</th>\n",
              "      <td>1848</td>\n",
              "      <td>3</td>\n",
              "      <td>in time I leadership role I think I great lead...</td>\n",
              "      <td>117</td>\n",
              "      <td>689</td>\n",
              "      <td>0</td>\n",
              "      <td>1485</td>\n",
              "      <td>4.252287</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5417</th>\n",
              "      <td>5417</td>\n",
              "      <td>3</td>\n",
              "      <td>yes immediately say just attention detail pape...</td>\n",
              "      <td>68</td>\n",
              "      <td>723</td>\n",
              "      <td>0</td>\n",
              "      <td>1656</td>\n",
              "      <td>4.706761</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1136</th>\n",
              "      <td>1136</td>\n",
              "      <td>3</td>\n",
              "      <td>anti - anxiety medication day able function so...</td>\n",
              "      <td>29</td>\n",
              "      <td>168</td>\n",
              "      <td>0</td>\n",
              "      <td>438</td>\n",
              "      <td>4.569014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2125</th>\n",
              "      <td>2125</td>\n",
              "      <td>3</td>\n",
              "      <td>youtubeangelsarah maclachlan youtubecom youtub...</td>\n",
              "      <td>36</td>\n",
              "      <td>252</td>\n",
              "      <td>4</td>\n",
              "      <td>773</td>\n",
              "      <td>5.241507</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1800 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Unnamed: 0  type                                              posts  \\\n",
              "3179        3179     0  aptain phillip happy nice I m currently read e...   \n",
              "914          914     0  lol that s I figure mean I like way I do not s...   \n",
              "1361        1361     0  believe balance important you want able enjoyg...   \n",
              "1539        1539     0  do not know come I great love christmas grow p...   \n",
              "7559        7559     0  to honest I do not know I truly capable want m...   \n",
              "...          ...   ...                                                ...   \n",
              "7200        7200     3  can not draw there s youtubecom is not obvious...   \n",
              "1848        1848     3  in time I leadership role I think I great lead...   \n",
              "5417        5417     3  yes immediately say just attention detail pape...   \n",
              "1136        1136     3  anti - anxiety medication day able function so...   \n",
              "2125        2125     3  youtubeangelsarah maclachlan youtubecom youtub...   \n",
              "\n",
              "      upperCount  stopWordCount  urlCount  wordCount  avgWordLen  \n",
              "3179          10            178         0        391    4.534834  \n",
              "914           91            617         0       1382    4.214347  \n",
              "1361          67            688         2       1473    4.566697  \n",
              "1539          88            554         0       1279    4.978543  \n",
              "7559         119            860         1       1783    4.223365  \n",
              "...          ...            ...       ...        ...         ...  \n",
              "7200          40            467         2       1061    4.666140  \n",
              "1848         117            689         0       1485    4.252287  \n",
              "5417          68            723         0       1656    4.706761  \n",
              "1136          29            168         0        438    4.569014  \n",
              "2125          36            252         4        773    5.241507  \n",
              "\n",
              "[1800 rows x 8 columns]"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df0 = df[df['type'] == 0].sample(450)\n",
        "df1 = df[df['type'] == 1].sample(450)\n",
        "df2 = df[df['type'] == 2].sample(450)\n",
        "df3 = df[df['type'] == 3].sample(450)\n",
        "\n",
        "dfr = pd.concat([df0,df1,df2,df3],)\n",
        "dfr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# dfr.to_csv(path_or_buf=\"./csv_files/dfr_spojeni.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y = dfr['type']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cv = CountVectorizer(dtype=np.uint8)\n",
        "text_counts = cv.fit_transform(dfr['posts'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text_counts.toarray().shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dfr_bow = pd.DataFrame(text_counts.toarray(),columns=cv.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dfr_bow.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Num. of words: \" + str(len(list(dfr_bow.columns))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dfr_features = dfr.drop(labels=['posts','type'],axis=1).reset_index(drop=True)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**this reset_index should be double checked**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# dfr_bow.to_csv(path_or_buf=\"./csv_files/dfr_bow.csv\")\n",
        "# dfr.to_csv(path_or_buf=\"./csv_files/dfr.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ML ALGORITHMS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score,classification_report\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rfc = RandomForestClassifier(random_state=42,n_estimators=500,n_jobs=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "def classify(X,y):\n",
        "  scaler = MinMaxScaler(feature_range=(0,1))\n",
        "  X=scaler.fit_transform(X)\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X,y,test_size= 0.2,random_state=42,stratify=y)\n",
        "\n",
        "  rfc.fit(X_train,y_train)\n",
        "  y_pred = rfc.predict(X_test)\n",
        "  ac = accuracy_score(y_test,y_pred)\n",
        "\n",
        "  ################ CONFUSION MATRIX ####################\n",
        "  # Get and reshape confusion matrix data\n",
        "  matrix = confusion_matrix(y_test, y_pred)\n",
        "  matrix = matrix.astype('float') / matrix.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "  # Build the plot\n",
        "  plt.figure(figsize=(16, 7))\n",
        "  sns.set(font_scale=1.4)\n",
        "  sns.heatmap(matrix, annot=True, annot_kws={'size': 10},\n",
        "              cmap=plt.cm.Greens, linewidths=0.2)\n",
        "\n",
        "  # Add labels to the plot\n",
        "  class_names = ['GUARDIAN', 'ARTISAN', 'IDEALIST', 'RATIONALIST']\n",
        "  tick_marks = np.arange(len(class_names))\n",
        "  tick_marks2 = tick_marks + 0.5\n",
        "  plt.xticks(tick_marks, class_names, rotation=25)\n",
        "  plt.yticks(tick_marks2, class_names, rotation=0)\n",
        "  plt.xlabel('Predicted label')\n",
        "  plt.ylabel('True label')\n",
        "  plt.title('Confusion Matrix for Random Forest Model')\n",
        "  plt.show()\n",
        "  ################ CONFUSION MATRIX ####################\n",
        "\n",
        "  print(\"accuracy: \",ac)\n",
        "  print(classification_report(y_test, y_pred))\n",
        "\n",
        "  ############### FEATURE IMPORTANCE ###################\n",
        "  features = scaler.get_feature_names_out()\n",
        "  fi = rfc.feature_importances_\n",
        "  importance = {features[i]:fi[i] for i in range(0, len(features))}\n",
        "  wc = WordCloud(width=800, height=400).generate_from_frequencies(importance)\n",
        "  plt.imshow(wc)\n",
        "  plt.axis('off')\n",
        "  plt.show()\n",
        "  ############### FEATURE IMPORTANCE ###################\n",
        "\n",
        "classify(dfr_features.join(dfr_bow),y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### average accuracy on 100 forests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def classify(X,y):\n",
        "#   scaler = MinMaxScaler(feature_range=(0,1))\n",
        "#   X=scaler.fit_transform(X)\n",
        "\n",
        "#   X_train, X_test, y_train, y_test = train_test_split(X,y,test_size= 0.2,random_state=42,stratify=y)\n",
        "\n",
        "#   rfc.fit(X_train,y_train)\n",
        "#   y_pred = rfc.predict(X_test)\n",
        "#   ac = accuracy_score(y_test,y_pred)\n",
        "\n",
        "#   return ac\n",
        "\n",
        "# test = []\n",
        "\n",
        "# for i in range(0,100):\n",
        "#   df0 = df[df['type'] == 0].sample(450)\n",
        "#   df1 = df[df['type'] == 1].sample(450)\n",
        "#   df2 = df[df['type'] == 2].sample(450)\n",
        "#   df3 = df[df['type'] == 3].sample(450)\n",
        "#   dfr = pd.concat([df0, df1, df2, df3],)\n",
        "#   y = dfr['type']\n",
        "#   cv = CountVectorizer(dtype='b')\n",
        "#   text_counts = cv.fit_transform(dfr['posts'])\n",
        "#   dfr_bow = pd.DataFrame(text_counts.toarray(),columns=cv.get_feature_names_out())\n",
        "#   dfr_features = dfr.drop(labels=['posts','type'],axis=1).reset_index(drop=True)  \n",
        "\n",
        "#   test.append(classify(dfr_features.join(dfr_bow), y))\n",
        "\n",
        "# print(sum(test)/len(test))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPipjDsvaeWPmm+I2AKTUv7",
      "collapsed_sections": [],
      "mount_file_id": "1_TEjG8vJVKTW0yXQp99YYob3Mif_LgsQ",
      "name": "NLPproject.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
